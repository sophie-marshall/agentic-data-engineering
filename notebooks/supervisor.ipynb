{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervisor Agent Constructuion/Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess\n",
    "\n",
    "# get root of current repo and add to our path\n",
    "root_dir = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL).decode(\"utf-8\").strip()\n",
    "\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Scanning Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_client_with_role(role_arn: str, aws_service: str, session_name: str, region: str = \"us-east-1\"):\n",
    "    # start sts session\n",
    "    sts = boto3.client(\"sts\")\n",
    "\n",
    "    # assume role + grab credentials\n",
    "    creds = sts.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=session_name\n",
    "    )[\"Credentials\"]\n",
    "\n",
    "    # create a new session with the assumed role credentials\n",
    "    return boto3.client(\n",
    "        aws_service,\n",
    "        aws_access_key_id=creds[\"AccessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"SecretAccessKey\"],\n",
    "        aws_session_token=creds[\"SessionToken\"],\n",
    "        region_name=region\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_yesterdays_files(s3_client):\n",
    "    today = datetime.today().strftime(\"%m%d%Y\")\n",
    "    yesterdays_files = []\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=\"agentic-de\", Prefix=f\"bronze/{today}/\"):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            yesterdays_files.append(key)\n",
    "    return yesterdays_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_meta(key, s3_client):\n",
    "    obj = s3_client.get_object(Bucket=\"agentic-de\", Key=key)\n",
    "    content = obj[\"Body\"].read(500).decode(\"utf-8\")\n",
    "    return f\"File: {key}\\nContent Preview:\\n{content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# create s3 client \n",
    "s3 = get_client_with_role(\n",
    "    role_arn=os.getenv(\"DIGI_INNO_ROLE_ARN\"), \n",
    "    aws_service=\"s3\",\n",
    "    session_name=\"digi-inno-s3\"\n",
    ")\n",
    "\n",
    "files = get_yesterdays_files(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Task Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.airflow import AirflowClient\n",
    "import os \n",
    "\n",
    "airflow = AirflowClient(\n",
    "    username=os.getenv(\"AIRFLOW_USERNAME\"),\n",
    "    password=os.getenv(\"AIRFLOW_PASSWORD\"),\n",
    ")\n",
    "\n",
    "airflow_res = airflow.trigger_dag(\"transform_pbs_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
