{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler Testing\n",
    "\n",
    "Test out our crawler and writing process before we toss it into Airflow. Envisioning this as two parallel crawls + 1 upsert event to S3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess\n",
    "\n",
    "# get root of current repo and add to our path\n",
    "root_dir = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL).decode(\"utf-8\").strip()\n",
    "\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Raw Data \n",
    "\n",
    "Let's continously search for new XML files and dump them in S3. We'll make this 2 jobs in parallel and we are making the choice not to worry about duplication here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.dags.utils.constants import FEED_URLS\n",
    "import requests\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def request_feed(rss_feed_url: str) -> dict[str, str] | None:\n",
    "    res = requests.get(rss_feed_url, timeout=10)\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        print(f\"Error retrieving XML document for {rss_feed_url}: {res.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"rss_url\": rss_feed_url,\n",
    "        \"xml_doc\": res.text,\n",
    "    }\n",
    "\n",
    "def get_s3_client_with_role(role_arn: str, region: str = \"us-east-1\") -> boto3.client:\n",
    "    sts = boto3.client(\"sts\")\n",
    "    creds = sts.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=\"airflow-dag-session\"\n",
    "    )[\"Credentials\"]\n",
    "\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=creds[\"AccessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"SecretAccessKey\"],\n",
    "        aws_session_token=creds[\"SessionToken\"],\n",
    "        region_name=region,\n",
    "    )\n",
    "\n",
    "def upload_to_bronze(res_dict: dict[str, str], s3_client) -> None:\n",
    "    today = datetime.today().strftime(\"%m%d%Y\")\n",
    "    try: \n",
    "        s3_client.put_object(\n",
    "            Bucket=\"agentic-de\", \n",
    "            Key=f\"bronze/{today}/{quote_plus(res_dict[\"rss_url\"])}_{today}.xml\",\n",
    "            Body=res_dict[\"xml_doc\"], \n",
    "            ContentType=\"application/xml\"\n",
    "        )\n",
    "        print(\"Sucess!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "s3 = get_s3_client_with_role(os.getenv(\"DIGI_INNO_ROLE_ARN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch page\n",
    "res_dict = request_feed(\"https://www.npr.org/rss/rss.php?id=1126\")\n",
    "\n",
    "# test upload\n",
    "upload_to_bronze(res_dict, s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
