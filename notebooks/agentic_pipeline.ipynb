{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agentic Pipeline \n",
    "\n",
    "Now that we have the individual pieces working, let's put it all together into a full Airflow pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess\n",
    "\n",
    "# get root of current repo and add to our path\n",
    "root_dir = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL).decode(\"utf-8\").strip()\n",
    "\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Extraction \n",
    "\n",
    "Content will be scraped from available public media RSS feeds. This job will be designed to run every night at 5PM (provided the server and scheduler are running) and will write outputs to the `agentic-de/bronze` data directory.\n",
    "\n",
    "The code blocks defined here will be consolidated into a single Airflow task in our Agentic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/Users/srmarshall/Desktop/code/personal/agentic-data-engineering/airflow/dags/utils/helpers.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">23</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> GuessedAtParserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: No parser was explicitly specified, so I'm using the best available HTML parser for this system </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">\"html.parser\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">. This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.</span>\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">The code that caused this warning is on line </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">23</span><span style=\"color: #808000; text-decoration-color: #808000\"> of the file /Users/srmarshall/Desktop/code/personal/agentic-data-engineering/airflow/dags/utils/helpers.py. To get rid of this warning, pass the additional argument </span><span style=\"color: #808000; text-decoration-color: #808000\">'features=\"html.parser\"'</span><span style=\"color: #808000; text-decoration-color: #808000\"> to the BeautifulSoup constructor.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/Users/srmarshall/Desktop/code/personal/agentic-data-engineering/airflow/dags/utils/\u001b[0m\u001b[1;33mhelpers.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m23\u001b[0m\u001b[1;33m GuessedAtParserWarning\u001b[0m\u001b[33m: No parser was explicitly specified, so I'm using the best available HTML parser for this system \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\u001b[0m\n",
       "\n",
       "\u001b[33mThe code that caused this warning is on line \u001b[0m\u001b[1;33m23\u001b[0m\u001b[33m of the file \u001b[0m\u001b[33m/Users/srmarshall/Desktop/code/personal/agentic-data-engineering/airflow/dags/utils/\u001b[0m\u001b[33mhelpers.py.\u001b[0m\u001b[33m To get rid of this warning, pass the additional argument \u001b[0m\u001b[33m'\u001b[0m\u001b[33mfeatures\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"html\u001b[0m\u001b[33m.parser\"'\u001b[0m\u001b[33m to the BeautifulSoup constructor.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to request 232 RSS feeds\n"
     ]
    }
   ],
   "source": [
    "from airflow.dags.utils.helpers import generate_npr_feed_urls\n",
    "\n",
    "# get RSS feeds from public media sources\n",
    "npr_rss_feeds = generate_npr_feed_urls()\n",
    "pbs_rss_feeds = [\n",
    "    \"https://www.pbs.org/newshour/feeds/rss/headlines\",\n",
    "    \"https://www.pbs.org/newshour/feeds/rss/politics\",\n",
    "    \"https://www.pbs.org/newshour/feeds/rss/brooks-and-capehart\"\n",
    "]\n",
    "\n",
    "# combine \n",
    "rss_feeds_to_crawl = npr_rss_feeds + pbs_rss_feeds\n",
    "\n",
    "# status update\n",
    "print(f\"Preparing to request {len(rss_feeds_to_crawl)} RSS feeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Requesting RSS feeds: 100%|██████████| 5/5 [00:02<00:00,  1.90feed/s]\n"
     ]
    }
   ],
   "source": [
    "from airflow.dags.utils.helpers import request_rss_feed\n",
    "import tqdm\n",
    "\n",
    "raw_feed_data = []\n",
    "for url in tqdm.tqdm(rss_feeds_to_crawl[:5], desc=\"Requesting RSS feeds\", unit=\"feed\"):\n",
    "    try:\n",
    "        feed_data = request_rss_feed(url)\n",
    "        if feed_data:\n",
    "            raw_feed_data.append(feed_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error requesting {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-06-11T15:48:21.249-0400\u001b[0m] {\u001b[34mcredentials.py:\u001b[0m1352} INFO\u001b[0m - Found credentials in shared credentials file: ~/.aws/credentials\u001b[0m\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from airflow.dags.utils.aws import S3 \n",
    "import os \n",
    "\n",
    "S3.upload_raw_rss_data(raw_feed_data[0], role_arn=os.getenv(\"DIGI_INNO_ROLE_ARN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation \n",
    "\n",
    "Here's where we'll embed our Agent! It will help us make an intelligent decision about which transformation pipeline a given file should be sent to. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
