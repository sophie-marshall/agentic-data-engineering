{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Experiments \n",
    "\n",
    "Right now, we ingest information using RSS feeds, can we do this in a more exhaustive manner?\n",
    "\n",
    "A conclusion from this process is that maybe agents really aren't the right choice here. At least for PBS content. The pages are well and consistently structured so our hardcoded rules allow our pipelines to run to completion. \n",
    "\n",
    "As we explore other sites with less reliable strucrues, maybe it will be something to consider but for now, the added complexity, in my mind, isn't worth it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBS_NEWSHOUR_BASE_URL = \"https://www.pbs.org/newshour\"\n",
    "\n",
    "HEADERS = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            + \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            + \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "sections = [\"latest\", \"politics\", \"arts\", \"nation\", \"world\", \"economy\", \"science\", \"health\", \"education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_section_urls(section: str, pages_to_search: int) -> list:\n",
    "    section_urls = []\n",
    "\n",
    "    for i in tqdm(range(1, pages_to_search + 1), desc=f\"Fetching {section.upper()} URLs\"):\n",
    "        # construct the URL for each page in the section\n",
    "        url = f\"{PBS_NEWSHOUR_BASE_URL}/{section}/page/{i}\"\n",
    "\n",
    "        page = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # set wrapper container\n",
    "        if section == \"latest\":\n",
    "            wrapper_class_name = \"latest__wrapper\"\n",
    "        else: \n",
    "            wrapper_class_name = \"archive__wrapper\"\n",
    "\n",
    "        # set link class name \n",
    "        if section == \"latest\":\n",
    "            link_class_name = \"card-timeline__title\"\n",
    "        elif section in [\"politics\", \"arts\"]:\n",
    "            link_class_name = \"card-horiz__title\"\n",
    "        else:\n",
    "            link_class_name = \"card-lg__title\"\n",
    "            \n",
    "        page_links = soup.find(\"div\", class_=wrapper_class_name).find_all(\"a\", class_=link_class_name)\n",
    "\n",
    "        section_urls.extend([link.get(\"href\") for link in page_links])\n",
    "\n",
    "    return section_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_page_data(url: str, section: str) -> dict:\n",
    "    page = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    slug = url.split(\"/\")[-1]\n",
    "\n",
    "    publication_date = soup.find(\"time\").get_text(strip=True)\n",
    "\n",
    "    byline = soup.find_all(\"a\", class_=\"post__byline-name-unhyphenated\")\n",
    "    authors = [author.find(\"span\").get_text(strip=True) for author in byline]\n",
    "\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    content = \" \".join([paragraph.get_text(strip=True) for paragraph in paragraphs])\n",
    "\n",
    "    main_classname = soup.find(\"main\").get(\"class\")[0]\n",
    "    if \"video\" in main_classname:\n",
    "        content_type = \"video\"\n",
    "    else:\n",
    "        content_type = \"article\"\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"slug\": slug,\n",
    "        \"publication_date\": publication_date,\n",
    "        \"authors\": authors,\n",
    "        \"content\": content,\n",
    "        \"meatadata\": {\n",
    "            \"content_type\": content_type,\n",
    "            \"section\": section, \n",
    "            \"short_description\": None, \n",
    "            \"tags\": None\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get List of Page URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching LATEST URLs: 100%|██████████| 20/20 [00:20<00:00,  1.04s/it]\n",
      "Fetching POLITICS URLs: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n",
      "Fetching ARTS URLs: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n",
      "Fetching NATION URLs: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n",
      "Fetching WORLD URLs: 100%|██████████| 20/20 [00:22<00:00,  1.15s/it]\n",
      "Fetching ECONOMY URLs: 100%|██████████| 20/20 [00:21<00:00,  1.07s/it]\n",
      "Fetching SCIENCE URLs: 100%|██████████| 20/20 [00:19<00:00,  1.02it/s]\n",
      "Fetching HEALTH URLs: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n",
      "Fetching EDUCATION URLs: 100%|██████████| 20/20 [00:18<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "all_urls = set()\n",
    "\n",
    "for section in sections:\n",
    "    pages_to_search = 20\n",
    "    section_urls = get_section_urls(section, pages_to_search)\n",
    "\n",
    "    all_urls.update(section_urls)\n",
    "\n",
    "all_urls = list(all_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data for Each Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching page data: 100%|██████████| 500/500 [16:48<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from time import sleep \n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for i, url in enumerate(tqdm(all_urls, desc=\"Fetching page data\")):\n",
    "    if i % 10 == 0 and i != 0:\n",
    "        sleep(15)\n",
    "\n",
    "    data = get_page_data(url, url.split(\"/\")[4])\n",
    "    all_data.append(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
